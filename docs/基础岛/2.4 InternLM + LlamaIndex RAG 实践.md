# 2.4 InternLM + LlamaIndex RAG å®è·µ

![alt text](image-26.png)

## ä¸€ã€ä»»åŠ¡è¯´æ˜

[ä»»åŠ¡åœ°å€](https://github.com/InternLM/Tutorial/blob/camp3/docs/L1/LlamaIndex/task.md)

### 1.åŸºç¡€ä»»åŠ¡

- åŸºäº LlamaIndex æ„å»ºè‡ªå·±çš„ RAG çŸ¥è¯†åº“

## äºŒã€ä»»åŠ¡æäº¤

### åŸºç¡€ä»»åŠ¡

- Xxtuneræ˜¯ä»€ä¹ˆï¼Ÿ

    - before 

        ![alt text](image-30.png)
    
    - after

        ![alt text](image-31.png)

- è‡ªå®šä¹‰é—®é¢˜åŠçŸ¥è¯†åº“

    ![alt text](image-33.png)


## ä¸‰ã€å¤ç°æ­¥éª¤

### æ¦‚å¿µ

  - æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval Augmented Generationï¼ŒRAGï¼‰

    - RAG æ˜¯ç›®å‰å¤§è¯­è¨€æ¨¡å‹ç›¸å…³æœ€çŸ¥åçš„å·¥å…·ä¹‹ä¸€ï¼Œä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢äº‹å®ï¼Œä»¥ä¾¿ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) æä¾›æœ€å‡†ç¡®ã€æœ€æ–°çš„ä¿¡æ¯ã€‚

  - æºè¯å‘é‡æ¨¡å‹ Sentence Transformer  (a.k.a. SBERT) æ˜¯ç”¨äºè®¿é—®ã€ä½¿ç”¨å’Œè®­ç»ƒæœ€å…ˆè¿›çš„æ–‡æœ¬å’Œå›¾åƒåµŒå…¥æ¨¡å‹çš„é¦–é€‰ Python æ¨¡å—

    - [æ–‡æ¡£](https://www.sbert.net/)

### æ­¥éª¤

- Step1 30% A100 å¼€å‘æœºåŠç¯å¢ƒ

    ![alt text](image-27.png)

- Step2 conda åˆ›å»ºç¯å¢ƒåŠpytorchå®‰è£…

    ``` conda create -n rag python=3.10 ```

    ``` conda activate rag ```

    ```conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia```

    ```pip install einops```
    
    ```pip install  protobuf ```

    - einopsæ˜¯ä¸€ä¸ªPythonåº“ï¼Œæä¾›äº†ç”¨äºæ·±åº¦å­¦ä¹ çš„é«˜çº§æ“ä½œï¼Œå¯ä»¥æ–¹ä¾¿åœ°å¯¹å¤šç»´æ•°æ®è¿›è¡Œæ“ä½œå’Œå˜æ¢ã€‚
    - protobufæ˜¯Googleå¼€å‘çš„ä¸€ç§æ•°æ®åºåˆ—åŒ–åè®®ï¼Œå¯ä»¥å°†æ•°æ®ç»“æ„ä»¥äºŒè¿›åˆ¶æ ¼å¼è¿›è¡Œåºåˆ—åŒ–å’Œååºåˆ—åŒ–ï¼Œç›¸æ¯”äºæ–‡æœ¬æ ¼å¼çš„æ•°æ®åºåˆ—åŒ–æ–¹å¼ï¼ˆå¦‚JSONï¼‰ï¼Œå…·æœ‰æ›´é«˜çš„æ•ˆç‡å’Œæ›´å°çš„ä½“ç§¯ã€‚

- Step3 Llamaindexã€transformersã€sentence-transformerså®‰è£…

    ```pip install llama-index==0.10.38 llama-index-llms-huggingface==0.2.0 "transformers[torch]==4.41.1" "huggingface_hub[inference]==0.23.1" huggingface_hub==0.23.1 sentence-transformers==2.7.0 sentencepiece==0.2.0```

- Step4 è¯å‘é‡æ¨¡å‹ä¸‹è½½ï¼ˆparaphrase-multilingual-MiniLM-L12-v2ï¼‰

    [paraphrase-multilingual-MiniLM-L12-v2 åœ°å€](https://huggingface.co/sentence-transformers/)

    - åˆ›å»ºragmodel.pyæ–‡ä»¶
        ```python 
        import os

        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

        # ä¸‹è½½æ¨¡å‹
        os.system('huggingface-cli download --resume-download sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 --local-dir /root/pro/model/sentence-transformer')
        ```
    - æ‰§è¡Œragmodel.pyæ–‡ä»¶
        ``` python /root/pro/ragmodel.py ```

        ![alt text](image-28.png)

- Step5 NLTKä¸‹è½½

    ``` bash 
        cd /root
        git clone https://gitee.com/yzy0612/nltk_data.git  --branch gh-pages
        cd nltk_data
        mv packages/*  ./
        cd tokenizers
        unzip punkt.zip
        cd ../taggers
        unzip averaged_perceptron_tagger.zip
    ```
    ![alt text](image-29.png)

- Step6 åŸºäºllamaindexå¯¹è¯

    - åˆ›å»º&è¿è¡Œllamaindex_chat.pyæ–‡ä»¶

    ```python
    from llama_index.llms.huggingface import HuggingFaceLLM
    from llama_index.core.llms import ChatMessage
    llm = HuggingFaceLLM(
        model_name="/root/model/internlm2-chat-1_8b",
        tokenizer_name="/root/model/internlm2-chat-1_8b",
        model_kwargs={"trust_remote_code":True},
        tokenizer_kwargs={"trust_remote_code":True}
    )

    rsp = llm.chat(messages=[ChatMessage(content="xtuneræ˜¯ä»€ä¹ˆï¼Ÿ")])
    print(rsp)
    ```

    ``` python /root/pro/llamaindex_chat.py ```

    ![alt text](image-30.png)

- Step7 åŸºäºllamaindexçŸ¥è¯†åº“å¯¹è¯

    - å®‰è£…embeddingåº“

    ``` pip install llama-index-embeddings-huggingface llama-index-embeddings-instructor ```

    - åˆ›å»º&è¿è¡Œllamaindex_rag.pyæ–‡ä»¶

    ```python
    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings

    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.llms.huggingface import HuggingFaceLLM

    #åˆå§‹åŒ–ä¸€ä¸ªHuggingFaceEmbeddingå¯¹è±¡ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º
    embed_model = HuggingFaceEmbedding(
    #æŒ‡å®šäº†ä¸€ä¸ªé¢„è®­ç»ƒçš„sentence-transformeræ¨¡å‹çš„è·¯å¾„
        model_name="/root/pro/model/sentence-transformer"
    )
    #å°†åˆ›å»ºçš„åµŒå…¥æ¨¡å‹èµ‹å€¼ç»™å…¨å±€è®¾ç½®çš„embed_modelå±æ€§ï¼Œ
    #è¿™æ ·åœ¨åç»­çš„ç´¢å¼•æ„å»ºè¿‡ç¨‹ä¸­å°±ä¼šä½¿ç”¨è¿™ä¸ªæ¨¡å‹ã€‚
    Settings.embed_model = embed_model

    llm = HuggingFaceLLM(
        model_name="/root/pro/Laboratory/internlm2-chat-1_8b",
        tokenizer_name="/root/pro/Laboratory/internlm2-chat-1_8b",
        model_kwargs={"trust_remote_code":True},
        tokenizer_kwargs={"trust_remote_code":True}
    )
    #è®¾ç½®å…¨å±€çš„llmå±æ€§ï¼Œè¿™æ ·åœ¨ç´¢å¼•æŸ¥è¯¢æ—¶ä¼šä½¿ç”¨è¿™ä¸ªæ¨¡å‹ã€‚
    Settings.llm = llm

    #ä»æŒ‡å®šç›®å½•è¯»å–æ‰€æœ‰æ–‡æ¡£ï¼Œå¹¶åŠ è½½æ•°æ®åˆ°å†…å­˜ä¸­
    documents = SimpleDirectoryReader("/root/pro/XTuner").load_data()
    #åˆ›å»ºä¸€ä¸ªVectorStoreIndexï¼Œå¹¶ä½¿ç”¨ä¹‹å‰åŠ è½½çš„æ–‡æ¡£æ¥æ„å»ºç´¢å¼•ã€‚
    # æ­¤ç´¢å¼•å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡ï¼Œå¹¶å­˜å‚¨è¿™äº›å‘é‡ä»¥ä¾¿äºå¿«é€Ÿæ£€ç´¢ã€‚
    index = VectorStoreIndex.from_documents(documents)
    # åˆ›å»ºä¸€ä¸ªæŸ¥è¯¢å¼•æ“ï¼Œè¿™ä¸ªå¼•æ“å¯ä»¥æ¥æ”¶æŸ¥è¯¢å¹¶è¿”å›ç›¸å…³æ–‡æ¡£çš„å“åº”ã€‚
    query_engine = index.as_query_engine()
    response = query_engine.query("xtuneræ˜¯ä»€ä¹ˆ?")

    print(response)    
    ```

    ![alt text](image-31.png)

- Step8 æ›¿æ¢çŸ¥è¯†åº“è¿›è¡Œå¯¹åº”é—®ç­”

    - [çŸ¥è¯†åº“githubåœ°å€](https://github.com/datawhalechina/llm-universe)
        ![alt text](image-32.png)

    - ä¿®æ”¹ä»£ç 
        - llamaindex_chat.py
        ```python
        rsp = llm.chat(messages=[ChatMessage(content="è¯·ä»‹ç»llm-universe ")])
        ```
        - llamaindex_rag.py
        ```python
        documents = SimpleDirectoryReader("/root/pro/llm-universe").load_data() 
        ```
    - æ‰§è¡Œllamaindex_chat.pyåŠllamaindex_rag.py
    ![alt text](image-33.png)

- Step9 webui

    - å®‰è£…streamlit
    ``` pip install streamlit ```

    - åˆ›å»º&è¿è¡Œllamaindexwebui.pyæ–‡ä»¶

    ```python
    import streamlit as st
    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    from llama_index.llms.huggingface import HuggingFaceLLM

    st.set_page_config(page_title="llama_index_demo", page_icon="ğŸ¦œğŸ”—")
    st.title("llama_index_demo")

    # åˆå§‹åŒ–æ¨¡å‹
    @st.cache_resource
    def init_models():
        embed_model = HuggingFaceEmbedding(
            model_name="/root/pro/model/sentence-transformer"
        )
        Settings.embed_model = embed_model

        llm = HuggingFaceLLM(
            model_name="/root/pro/Laboratory/internlm2-chat-1_8b",
            tokenizer_name="/root/pro/Laboratory/internlm2-chat-1_8b",
            model_kwargs={"trust_remote_code": True},
            tokenizer_kwargs={"trust_remote_code": True}
        )
        Settings.llm = llm

        documents = SimpleDirectoryReader("/root/pro/llm-universe").load_data()
        index = VectorStoreIndex.from_documents(documents)
        query_engine = index.as_query_engine()

        return query_engine

    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–æ¨¡å‹
    if 'query_engine' not in st.session_state:
        st.session_state['query_engine'] = init_models()

    def greet2(question):
        response = st.session_state['query_engine'].query(question)
        return response

        
    # Store LLM generated responses
    if "messages" not in st.session_state.keys():
        st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„llm-universeå­¦ä¹ åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}]    

        # Display or clear chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    def clear_chat_history():
        st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„llm-universeåŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}]

    st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

    # Function for generating LLaMA2 response
    def generate_llama_index_response(prompt_input):
        return greet2(prompt_input)

    # User-provided prompt
    if prompt := st.chat_input():
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

    # Gegenerate_llama_index_response last message is not from assistant
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = generate_llama_index_response(prompt)
                placeholder = st.empty()
                placeholder.markdown(response)
        message = {"role": "assistant", "content": response}
        st.session_state.messages.append(message)
    ```

    - æ‰§è¡Œllamaindexwebui.py

    ``` streamlit run /root/pro/llamaindexwebui.py ```

    ![alt text](image-34.png)

## å››ã€ æ‰©å±•

### 4.1 embeddingæ¨¡å‹

- [Sentence Transformer](https://www.sbert.net/)
- [Huggingface of Sentence Transformer](https://huggingface.co/sentence-transformers)
- ollama nomic-embed-text
- [æ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard)

### 4.2 LlamaIndex 

- [LlamaIndex github](https://github.com/run-llama/llama_index)