# 2.4 InternLM + LlamaIndex RAG 实践

![alt text](image-26.png)

## 一、任务说明

[任务地址](https://github.com/InternLM/Tutorial/blob/camp3/docs/L1/LlamaIndex/task.md)

### 1.基础任务

- 基于 LlamaIndex 构建自己的 RAG 知识库

## 二、任务提交

### 基础任务

## 三、复现步骤

### 概念

  - 检索增强生成（Retrieval Augmented Generation，RAG）

    - RAG 是目前大语言模型相关最知名的工具之一，从外部知识库中检索事实，以便为大型语言模型 (LLM) 提供最准确、最新的信息。

  - 源词向量模型 Sentence Transformer  (a.k.a. SBERT) 是用于访问、使用和训练最先进的文本和图像嵌入模型的首选 Python 模块

    - [文档](https://www.sbert.net/)

### 步骤

- Step1 30% A100 开发机及环境

    ![alt text](image-27.png)

- Step2 conda 创建环境及pytorch安装

    ``` conda create -n rag python=3.10 ```

    ``` conda activate rag ```

    ```conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia```

    ```pip install einops```
    
    ```pip install  protobuf ```

    - einops是一个Python库，提供了用于深度学习的高级操作，可以方便地对多维数据进行操作和变换。
    - protobuf是Google开发的一种数据序列化协议，可以将数据结构以二进制格式进行序列化和反序列化，相比于文本格式的数据序列化方式（如JSON），具有更高的效率和更小的体积。

- Step3 Llamaindex、transformers、sentence-transformers安装

    ```pip install llama-index==0.10.38 llama-index-llms-huggingface==0.2.0 "transformers[torch]==4.41.1" "huggingface_hub[inference]==0.23.1" huggingface_hub==0.23.1 sentence-transformers==2.7.0 sentencepiece==0.2.0```

- Step4 词向量模型下载（paraphrase-multilingual-MiniLM-L12-v2）

    [paraphrase-multilingual-MiniLM-L12-v2 地址](https://huggingface.co/sentence-transformers/)

    - 创建ragmodel.py文件
        ```python 
        import os

        # 设置环境变量
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

        # 下载模型
        os.system('huggingface-cli download --resume-download sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 --local-dir /root/pro/model/sentence-transformer')
        ```
    - 执行ragmodel.py文件
        ``` python /root/pro/ragmodel.py ```

        ![alt text](image-28.png)

- Step5 NLTK下载

    ``` bash 
        cd /root
        git clone https://gitee.com/yzy0612/nltk_data.git  --branch gh-pages
        cd nltk_data
        mv packages/*  ./
        cd tokenizers
        unzip punkt.zip
        cd ../taggers
        unzip averaged_perceptron_tagger.zip
    ```
    ![alt text](image-29.png)

- Step6 基于llamaindex对话

    创建llamaindex_chat.py文件

    ```python
    from llama_index.llms.huggingface import HuggingFaceLLM
    from llama_index.core.llms import ChatMessage
    llm = HuggingFaceLLM(
        model_name="/root/model/internlm2-chat-1_8b",
        tokenizer_name="/root/model/internlm2-chat-1_8b",
        model_kwargs={"trust_remote_code":True},
        tokenizer_kwargs={"trust_remote_code":True}
    )

    rsp = llm.chat(messages=[ChatMessage(content="xtuner是什么？")])
    print(rsp)
    ```

    ![alt text](image-30.png)

## 四、 扩展



### embedding模型

- [Sentence Transformer](https://www.sbert.net/)
- [Huggingface of Sentence Transformer](https://huggingface.co/sentence-transformers)
- ollama nomic-embed-text
- (排行榜)[https://huggingface.co/spaces/mteb/leaderboard]
